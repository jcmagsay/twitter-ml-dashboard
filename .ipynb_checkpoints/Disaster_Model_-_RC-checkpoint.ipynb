{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use logistic regression to see what features are important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/perry.oconnor/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/perry.oconnor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages (0.5.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk.tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "nltk.download('wordnet')\n",
    "tknzr = TweetTokenizer()\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "nltk.download('punkt')\n",
    "!{sys.executable} -m pip install emoji\n",
    "import emoji\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "\n",
    "#from spellchecker import SpellChecker\n",
    "#!{sys.executable} -m pip install spellchecker\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-42d819a119c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msub_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/submission_set.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_disaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/jupyterlab/1.2.4/libexec/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/raw/train.csv\", encoding='utf8')\n",
    "test = pd.read_csv(\"./data/raw/test.csv\", encoding='utf8')\n",
    "sub_sample = pd.read_csv(\"./data/raw/submission_set.csv\", encoding='utf8')\n",
    "\n",
    "train_disaster = pd.read_csv(\"./data/raw/train.csv\", encoding='utf8')\n",
    "train_disaster.drop(train_disaster[(train_disaster.disaster != 1)].index, axis=0,inplace=True)\n",
    "\n",
    "train_nondisaster = pd.read_csv(\"./data/raw/train.csv\", encoding='utf8')\n",
    "train_nondisaster.drop(train_nondisaster[(train_nondisaster.disaster != 0)].index, axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_processor:\n",
    "    def clean_tweets(self, df):\n",
    "        df_temp = df.copy()\n",
    "        df_temp['clean_text'] = df_temp['text'].apply(self.remove_mentions)\n",
    "        df_temp['clean_text'] = df_temp['clean_text'].apply(lambda x: x.lower())\n",
    "        df_temp['clean_text'] = df_temp['clean_text'].apply(self.remove_mentions)\n",
    "        df_temp['clean_text'] = df_temp['clean_text'].apply(self.remove_punctuations)\n",
    "        df_temp['clean_text'] = df_temp['clean_text'].apply(self.remove_urls)\n",
    "\n",
    "        return df_temp        \n",
    "    \n",
    "    def text_has_emoji(self, text):\n",
    "        for character in text:\n",
    "            if character in emoji.UNICODE_EMOJI:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "    def strip_emoji(self, text):\n",
    "        return RE_EMOJI.sub(r'',text)\n",
    "\n",
    "    # print(strip_emoji('helloðŸ™„ðŸ¤”'))\n",
    "\n",
    "    def remove_punctuations(self, text):\n",
    "        for punctuation in string.punctuation:\n",
    "            text = text.replace(punctuation, '')\n",
    "        return text\n",
    "\n",
    "    def remove_digits(self, text):\n",
    "        for digit in string.digits:\n",
    "            text = text.replace(digit, '')\n",
    "        return text\n",
    "\n",
    "    def remove_mentions(self, text):\n",
    "        word_list = text.split()\n",
    "        for word in word_list:\n",
    "            if word[0] == \"@\":\n",
    "                text = text.replace(word,'')\n",
    "        return text\n",
    "    # string = \"@soundcloud can you recover\"\n",
    "    # print(remove_mentions(string))\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        word_list = text.split()\n",
    "        for word in word_list:\n",
    "            if \"http\" in word:\n",
    "                text = text.replace(word,'')\n",
    "        return text\n",
    "\n",
    "    def contains_proper_nouns(self, text):\n",
    "        word_list = text.split()\n",
    "        word_list2 = word_list[1:]\n",
    "        for word in word_list2:\n",
    "            is_proper_noun = re.match(r'^[A-Z]',word)\n",
    "            if is_proper_noun:\n",
    "                return True\n",
    "\n",
    "    # string = \"hello this is a Test for Perry\"\n",
    "    # print(num_proper_nouns(string))\n",
    "\n",
    "    def has_lengthening(self, text):\n",
    "        pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "        text2 = pattern.sub(r\"\\1\\1\",text)\n",
    "        if text != text2:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # FEATURES\n",
    "    \n",
    "    def features(self, df):\n",
    "        df_temp = df.copy()\n",
    "        df_temp = self.contains_punctuation(df_temp)\n",
    "        df_temp = self.contains_multi_sentence(df_temp)\n",
    "        df_temp = self.contains_mention(df_temp)\n",
    "        df_temp = self.contains_disaster_word_clean(df_temp)\n",
    "        df_temp = self.contains_numbers(df_temp)\n",
    "        df_temp = self.contains_emoji(df_temp)\n",
    "        df_temp = self.contains_proper_noun(df_temp)\n",
    "        df_temp = self.has_lengthened_words(df_temp)\n",
    "        df_temp = self.contains_url(df_temp)\n",
    "        \n",
    "        return df_temp\n",
    "\n",
    "    \n",
    "    def contains_punctuation(self,df):\n",
    "        df_temp = df.copy()\n",
    "        nmbr_of_punct_marks = df_temp['text'].apply(lambda x: len(set(x) & set(string.punctuation)))\n",
    "        df_temp['feature_has_punctuation'] = nmbr_of_punct_marks.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "    def contains_multi_sentence(self,df):\n",
    "        df_temp = df.copy()\n",
    "        df_temp['clean_text'] = df_temp['text'].apply(self.remove_urls)\n",
    "        sentence_markers = [\".\",\"!\",\"?\"]\n",
    "        nmbr_of_sentences = df_temp['clean_text'].apply(lambda x: len(set(x) & set(sentence_markers)))\n",
    "        df_temp['feature_has_multi_sentence'] = nmbr_of_sentences.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "    def contains_mention(self, df):\n",
    "        df_temp = df.copy()\n",
    "        nmbr_of_mentions = df_temp['text'].apply(lambda x: len(set(x) & set(\"@\")))\n",
    "        df_temp['feature_has_mention'] = nmbr_of_mentions.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "#     def tweet_contains_disaster_word(self, df):\n",
    "#         df_temp = df.copy()\n",
    "#         disaster_words = [\"flood\",\"hurricane\",\"tornado\",\"fire\"]\n",
    "#         nmbr_of_disasters = df_temp['text'].apply(lambda x: len(set(x.split()) & set(disaster_words)))\n",
    "#         df_temp['feature_has_disaster_word'] = nmbr_of_disasters.astype(bool).astype(int)\n",
    "#         return df_temp\n",
    "\n",
    "    def contains_disaster_word_clean(self, df):\n",
    "        df_temp = df.copy()\n",
    "        df_temp = self.clean_tweets(df_temp)\n",
    "        disaster_words = [\"flood\",\"hurricane\",\"tornado\",\"fire\",\"earthquake\",\"cyclone\"]\n",
    "        nmbr_of_disasters = df_temp['clean_text'].apply(lambda x: len(set(x.split()) & set(disaster_words)))\n",
    "        df_temp['feature_has_disaster_word'] = nmbr_of_disasters.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "    def contains_numbers(self, df):\n",
    "        df_temp = df.copy()\n",
    "        nmbr_of_numbers = df_temp['text'].apply(lambda x: len(set(x) & set(string.digits)))\n",
    "        df_temp['feature_has_numbers'] = nmbr_of_numbers.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "    def contains_emoji(self, df):\n",
    "        df_temp = df.copy()\n",
    "        nmbr_of_emojis = df_temp['text'].apply(self.text_has_emoji)\n",
    "        df_temp['feature_has_emoji'] = nmbr_of_emojis.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "    def contains_proper_noun(self, df):\n",
    "        df_temp = df.copy()\n",
    "        df_temp['clean_text'] = df_temp['text'].apply(self.remove_mentions)\n",
    "        proper_nouns = df_temp['clean_text'].apply(self.contains_proper_nouns)\n",
    "        df_temp['feature_has_proper_nouns'] = proper_nouns.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "    def has_lengthened_words(self, df):\n",
    "        df_temp = df.copy()\n",
    "        df_temp['clean_text'] = df_temp['text'].apply(self.remove_punctuations)\n",
    "        df_temp['clean_text'] = df_temp['clean_text'].apply(lambda x: x.lower())\n",
    "\n",
    "        df_temp['clean_text'] = df_temp['clean_text'].apply(self.remove_digits)\n",
    "        df_temp['clean_text'] = df_temp['clean_text'].apply(self.remove_mentions)\n",
    "\n",
    "        has_lengthened_words = df_temp['clean_text'].apply(self.has_lengthening)\n",
    "        df_temp['feature_has_lengthened_words'] = has_lengthened_words.astype(bool).astype(int)\n",
    "        return df_temp\n",
    "\n",
    "    def contains_url(self, df):\n",
    "        df_temp = df.copy()\n",
    "        nmbr_of_urls = df_temp['text'].apply(lambda x: len(re.findall(\"http\",x)))\n",
    "        df_temp['feature_has_url'] = nmbr_of_urls.astype(bool).astype(int)\n",
    "        return df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = tweet_processor()\n",
    "df_features = processor.features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>text</th>\n",
       "      <th>disaster_type</th>\n",
       "      <th>disaster</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>feature_has_punctuation</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>feature_has_multi_sentence</th>\n",
       "      <th>feature_has_mention</th>\n",
       "      <th>feature_has_disaster_word</th>\n",
       "      <th>feature_has_numbers</th>\n",
       "      <th>feature_has_emoji</th>\n",
       "      <th>feature_has_proper_nouns</th>\n",
       "      <th>feature_has_lengthened_words</th>\n",
       "      <th>feature_has_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>@TheEllenShow Please check into Salt River hor...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>theellenshow please check into salt river hors...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>As for the hurricane, it's already category 1 ...</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>as for the hurricane its already category  and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10003</td>\n",
       "      <td>So it looks like my @SoundCloud profile shall ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>so it looks like my soundcloud profile shall b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10004</td>\n",
       "      <td>@SushmaSwaraj Am sure background check of the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>sushmaswaraj am sure background check of the c...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10005</td>\n",
       "      <td>Open forex detonation indicator is irretrievab...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>open forex detonation indicator is irretrievab...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweetid                                               text disaster_type  \\\n",
       "0    10001  @TheEllenShow Please check into Salt River hor...           NaN   \n",
       "1    10002  As for the hurricane, it's already category 1 ...     hurricane   \n",
       "2    10003  So it looks like my @SoundCloud profile shall ...           NaN   \n",
       "3    10004  @SushmaSwaraj Am sure background check of the ...           NaN   \n",
       "4    10005  Open forex detonation indicator is irretrievab...           NaN   \n",
       "\n",
       "   disaster  Unnamed: 4  feature_has_punctuation  \\\n",
       "0         0         NaN                        1   \n",
       "1         1         NaN                        1   \n",
       "2         0         NaN                        1   \n",
       "3         0         NaN                        1   \n",
       "4         0         NaN                        1   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  theellenshow please check into salt river hors...   \n",
       "1  as for the hurricane its already category  and...   \n",
       "2  so it looks like my soundcloud profile shall b...   \n",
       "3  sushmaswaraj am sure background check of the c...   \n",
       "4  open forex detonation indicator is irretrievab...   \n",
       "\n",
       "   feature_has_multi_sentence  feature_has_mention  feature_has_disaster_word  \\\n",
       "0                           1                    1                          0   \n",
       "1                           1                    0                          1   \n",
       "2                           1                    1                          0   \n",
       "3                           1                    1                          0   \n",
       "4                           0                    0                          0   \n",
       "\n",
       "   feature_has_numbers  feature_has_emoji  feature_has_proper_nouns  \\\n",
       "0                    1                  0                         1   \n",
       "1                    1                  0                         1   \n",
       "2                    0                  0                         1   \n",
       "3                    0                  0                         0   \n",
       "4                    1                  0                         1   \n",
       "\n",
       "   feature_has_lengthened_words  feature_has_url  \n",
       "0                             0                0  \n",
       "1                             0                0  \n",
       "2                             0                0  \n",
       "3                             0                0  \n",
       "4                             0                1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class contains_url(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def contains_url(self, text):\n",
    "        nmbr_of_urls = len(re.findall(\"http\",text))\n",
    "        return nmbr_of_urls.astype(bool).astype(int)\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"The workhorse of this feature extractor\"\"\"\n",
    "        return df['feature_contains_url'].apply(self.contains_url)\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      2331\n",
      "           1       0.83      0.99      0.91       473\n",
      "\n",
      "    accuracy                           0.97      2804\n",
      "   macro avg       0.92      0.98      0.94      2804\n",
      "weighted avg       0.97      0.97      0.97      2804\n",
      "\n",
      "[[2237   94]\n",
      " [   3  470]]\n",
      "0.9654065620542083\n"
     ]
    }
   ],
   "source": [
    "# build the pipeline\n",
    "ppl = Pipeline([\n",
    "              ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='char')),\n",
    "              ('clf',   ComplementNB())\n",
    "      ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('feats', FeatureUnion([\n",
    "        ('ngram', CountVectorizer(ngram_range=(1, 4), analyzer='char')), # can pass in either a pipeline\n",
    "        ('ave', contains_url()) # or a transformer\n",
    "    ])),\n",
    "    ('clf', ComplementNB())  # classifier\n",
    "])\n",
    "\n",
    "msg_train2, msg_test2, label_train2, label_test2 = train_test_split(train['text'], \n",
    "                                                                    train['disaster'], \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    random_state=30, \n",
    "                                                                    stratify=train['disaster'])\n",
    "\n",
    "# train the classifier\n",
    "model = ppl.fit(msg_train2,label_train2)\n",
    "\n",
    "# test the classifier\n",
    "y_test = model.predict(msg_test2)\n",
    "\n",
    "print(classification_report(y_test,label_test2))\n",
    "print(confusion_matrix(y_test,label_test2))\n",
    "print(accuracy_score(y_test,label_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train2, msg_test2, label_train2, label_test2 = train_test_split(train['text'], train['disaster'], test_size=0.2, random_state=30, stratify=train['disaster'])\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(msg_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homemade_pipeline(msg_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homemade_pipeline(data):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(data)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    return X_train_tfidf\n",
    "\n",
    "clf = ComplementNB().fit(homemade_pipeline(msg_train2), label_train2)\n",
    "predicted = clf.predict(homemade_pipeline(msg_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline.fit(msg_train2,label_train2)\n",
    "predictions2 = pipeline2.predict(msg_test2)\n",
    "print(classification_report(predictions2,label_test2))\n",
    "print(confusion_matrix(predictions2,label_test2))\n",
    "print(accuracy_score(predictions2,label_test2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
